{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWQL3BI_B5WW"
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39286,
     "status": "ok",
     "timestamp": 1737190461120,
     "user": {
      "displayName": "Nobita",
      "userId": "09065444703346289193"
     },
     "user_tz": -330
    },
    "id": "wm6hUX-KC9vg",
    "outputId": "07f61c78-dc9c-49dc-978d-130beecc6db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating kaggle with google collab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFaltNIpD6iQ"
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iH44_rdEJ0M"
   },
   "outputs": [],
   "source": [
    "!mv /content/drive/MyDrive/Notebooks/kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1543,
     "status": "ok",
     "timestamp": 1740424160486,
     "user": {
      "displayName": "Nobita",
      "userId": "09065444703346289193"
     },
     "user_tz": -330
    },
    "id": "HT0RY25hEZfb",
    "outputId": "caade55a-3777-4f28-ff28-fd312349a3be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                                     title                                              size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
      "------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
      "asinow/car-price-dataset                                Car Price Dataset                                 135KB  2025-01-26 19:53:28          16645        264  1.0              \n",
      "mahmoudelhemaly/students-grading-dataset                Student Performance & Behavior Dataset            508KB  2025-02-17 17:38:46           1521         34  1.0              \n",
      "vinothkannaece/sales-dataset                            sales dataset                                      27KB  2025-02-18 05:13:42           2395         41  1.0              \n",
      "abdulmalik1518/mobiles-dataset-2025                     Mobiles Dataset (2025)                             20KB  2025-02-18 06:50:24           1818         40  1.0              \n",
      "adilshamim8/education-and-career-success                Education & Career Success.                       118KB  2025-02-03 05:24:20           4426         70  1.0              \n",
      "adilshamim8/startup-growth-and-investment-data          Startup Growth & Investment Data                  141KB  2025-02-02 07:27:03           1248         24  1.0              \n",
      "adilshamim8/sleep-cycle-and-productivity                Sleep Cycle & Productivity                        155KB  2025-02-07 05:44:59           2666         44  1.0              \n",
      "akxiit/blinkit-sales-dataset                            Blinkit Sales Dataset                               1MB  2025-02-09 12:39:05           2329         34  0.9411765        \n",
      "mzohaibzeeshan/google-stock-price-data-2020-2025-googl  Google Stock Price Data (2020-2025) | GOOGL        36KB  2025-02-16 06:56:24           1286         32  1.0              \n",
      "adilshamim8/global-traffic-accidents-dataset            Global Traffic Accidents Dataset                  292KB  2025-02-06 08:18:10           1484         27  1.0              \n",
      "adilshamim8/daily-food-and-nutrition-dataset            Daily Food & Nutrition Dataset                    249KB  2025-02-05 06:25:32           1160         23  1.0              \n",
      "meharshanali/nvidia-stocks-data-2025                    NVIDIA Stocks Data 2025                           165KB  2025-02-17 06:37:58           1024         26  1.0              \n",
      "anandshaw2001/netflix-movies-and-tv-shows               Netflix Movies and TV Shows                         1MB  2025-01-03 10:33:01          21713        522  1.0              \n",
      "krishnanshverma/imdb-movies-dataset                     IMDb Movies Dataset                                38KB  2025-02-12 17:53:40           1243         32  1.0              \n",
      "arnavgupta1205/usa-housing-dataset                      USA Housing Dataset                                 6KB  2025-02-05 10:28:13            985         23  1.0              \n",
      "asinow/laptop-price-dataset                             Laptop Price Dataset                              181KB  2025-02-01 04:20:16           4035         82  1.0              \n",
      "ruchikakumbhar/zomato-dataset                           Zomato Dataset                                      2KB  2025-01-21 03:59:39           5423         96  1.0              \n",
      "flynn28/european-football-matches                       European Football Matches                           3MB  2025-02-19 23:57:57           1188         29  1.0              \n",
      "asinow/schizohealth-dataset                             Schizophrenia Dataset                             134KB  2025-02-08 21:13:46           1533         38  1.0              \n",
      "mahatiratusher/stroke-risk-prediction-dataset           Stroke Risk Prediction Dataset Based on Symptoms  454KB  2025-02-15 07:20:17            763         32  0.9411765        \n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFegcNdgJdVs"
   },
   "outputs": [],
   "source": [
    "! chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48239,
     "status": "ok",
     "timestamp": 1740424309496,
     "user": {
      "displayName": "Nobita",
      "userId": "09065444703346289193"
     },
     "user_tz": -330
    },
    "id": "GsCm_8wLJHZY",
    "outputId": "6f2e6326-e696-43d1-d51c-c7fc68f0d299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/geolek/grayscale-face-images\n",
      "License(s): MIT\n",
      "Downloading grayscale-face-images.zip to /content\n",
      "100% 4.65G/4.65G [00:46<00:00, 171MB/s]\n",
      "100% 4.65G/4.65G [00:46<00:00, 106MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download geolek/grayscale-face-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9432,
     "status": "ok",
     "timestamp": 1740424932389,
     "user": {
      "displayName": "Nobita",
      "userId": "09065444703346289193"
     },
     "user_tz": -330
    },
    "id": "_JN-qQiHZqJ9",
    "outputId": "7453b741-e6ef-4549-98a9-ecd484998e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  grayscale-face-images.zip\n",
      "replace Grayscale Face images/test/anger/anger001.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip grayscale-face-images.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItcNZ8h1tHNg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFFd-1njtexO"
   },
   "outputs": [],
   "source": [
    "# Path to the dataset folder\n",
    "base_dir = \"/content/Grayscale Face images/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1556,
     "status": "ok",
     "timestamp": 1740426002009,
     "user": {
      "displayName": "Nobita",
      "userId": "09065444703346289193"
     },
     "user_tz": -330
    },
    "id": "osfM6T3jt_2s",
    "outputId": "3e69b8eb-9ca9-42e4-f06e-093aef1a32f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72038 images belonging to 8 classes.\n",
      "Found 18005 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# Emotion label mapping\n",
    "emotion_map = {\n",
    "    0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy',\n",
    "    4: 'Sad', 5: 'Surprise', 6: 'Neutral'\n",
    "}\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Split into training and validation\n",
    ")\n",
    "\n",
    "# Load training and validation data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    # First convolutional layer\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Second convolutional layer\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Third convolutional layer\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Flattening and dense layers\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(train_generator.class_indices), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3023701,
     "status": "ok",
     "timestamp": 1740431389634,
     "user": {
      "displayName": "Nobita",
      "userId": "09065444703346289193"
     },
     "user_tz": -330
    },
    "id": "rEVa823aukoT",
    "outputId": "6d4f5e4c-b51e-44d0-df7f-efe90d64129c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 190ms/step - accuracy: 0.1686 - loss: 2.0484 - val_accuracy: 0.2688 - val_loss: 1.9125\n",
      "Epoch 2/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 164ms/step - accuracy: 0.2816 - loss: 1.8835 - val_accuracy: 0.3156 - val_loss: 1.7829\n",
      "Epoch 3/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 159ms/step - accuracy: 0.3306 - loss: 1.7854 - val_accuracy: 0.3493 - val_loss: 1.7144\n",
      "Epoch 4/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 159ms/step - accuracy: 0.3516 - loss: 1.7296 - val_accuracy: 0.3403 - val_loss: 1.7143\n",
      "Epoch 5/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 157ms/step - accuracy: 0.3680 - loss: 1.7054 - val_accuracy: 0.3725 - val_loss: 1.6681\n",
      "Epoch 6/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 155ms/step - accuracy: 0.3761 - loss: 1.6802 - val_accuracy: 0.3733 - val_loss: 1.6453\n",
      "Epoch 7/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 154ms/step - accuracy: 0.3848 - loss: 1.6574 - val_accuracy: 0.3808 - val_loss: 1.6299\n",
      "Epoch 8/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 154ms/step - accuracy: 0.3847 - loss: 1.6517 - val_accuracy: 0.3989 - val_loss: 1.6000\n",
      "Epoch 9/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 157ms/step - accuracy: 0.3982 - loss: 1.6311 - val_accuracy: 0.3893 - val_loss: 1.5928\n",
      "Epoch 10/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 154ms/step - accuracy: 0.3986 - loss: 1.6253 - val_accuracy: 0.4033 - val_loss: 1.5760\n",
      "Epoch 11/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 155ms/step - accuracy: 0.4058 - loss: 1.6159 - val_accuracy: 0.4026 - val_loss: 1.5726\n",
      "Epoch 12/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 154ms/step - accuracy: 0.4078 - loss: 1.6080 - val_accuracy: 0.4199 - val_loss: 1.5513\n",
      "Epoch 13/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 160ms/step - accuracy: 0.4081 - loss: 1.5968 - val_accuracy: 0.4147 - val_loss: 1.5542\n",
      "Epoch 14/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 154ms/step - accuracy: 0.4126 - loss: 1.5904 - val_accuracy: 0.4237 - val_loss: 1.5384\n",
      "Epoch 15/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 154ms/step - accuracy: 0.4137 - loss: 1.5805 - val_accuracy: 0.4172 - val_loss: 1.5471\n",
      "Epoch 16/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 163ms/step - accuracy: 0.4180 - loss: 1.5801 - val_accuracy: 0.4208 - val_loss: 1.5307\n",
      "Epoch 17/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 154ms/step - accuracy: 0.4204 - loss: 1.5727 - val_accuracy: 0.4262 - val_loss: 1.5231\n",
      "Epoch 18/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 154ms/step - accuracy: 0.4232 - loss: 1.5664 - val_accuracy: 0.4352 - val_loss: 1.5228\n",
      "Epoch 19/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 158ms/step - accuracy: 0.4248 - loss: 1.5608 - val_accuracy: 0.4305 - val_loss: 1.5154\n",
      "Epoch 20/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 155ms/step - accuracy: 0.4245 - loss: 1.5618 - val_accuracy: 0.4269 - val_loss: 1.5275\n",
      "Epoch 21/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 154ms/step - accuracy: 0.4261 - loss: 1.5528 - val_accuracy: 0.4384 - val_loss: 1.4967\n",
      "Epoch 22/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 155ms/step - accuracy: 0.4296 - loss: 1.5519 - val_accuracy: 0.4462 - val_loss: 1.4933\n",
      "Epoch 23/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 155ms/step - accuracy: 0.4314 - loss: 1.5491 - val_accuracy: 0.4409 - val_loss: 1.4891\n",
      "Epoch 24/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 155ms/step - accuracy: 0.4309 - loss: 1.5426 - val_accuracy: 0.4413 - val_loss: 1.4960\n",
      "Epoch 25/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 154ms/step - accuracy: 0.4345 - loss: 1.5406 - val_accuracy: 0.4422 - val_loss: 1.4916\n",
      "Epoch 26/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 161ms/step - accuracy: 0.4356 - loss: 1.5357 - val_accuracy: 0.4431 - val_loss: 1.4928\n",
      "Epoch 27/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 156ms/step - accuracy: 0.4337 - loss: 1.5432 - val_accuracy: 0.4389 - val_loss: 1.4878\n",
      "Epoch 28/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 154ms/step - accuracy: 0.4361 - loss: 1.5387 - val_accuracy: 0.4442 - val_loss: 1.4953\n",
      "Epoch 29/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 155ms/step - accuracy: 0.4350 - loss: 1.5345 - val_accuracy: 0.4497 - val_loss: 1.4774\n",
      "Epoch 30/30\n",
      "\u001b[1m1126/1126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 155ms/step - accuracy: 0.4384 - loss: 1.5293 - val_accuracy: 0.4503 - val_loss: 1.4697\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=30,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36579,
     "status": "ok",
     "timestamp": 1740431682841,
     "user": {
      "displayName": "Nobita",
      "userId": "09065444703346289193"
     },
     "user_tz": -330
    },
    "id": "3iMBlncBvjIY",
    "outputId": "c2266138-cb0a-4214-ecf6-d77c83583d02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.45\n",
      "Model saved as 'face_emotion_detection_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_result = model.evaluate(validation_generator, verbose=0)\n",
    "print(f\"Validation Accuracy: {eval_result[1]:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"face_model.h5\")\n",
    "print(\"Model saved as 'face_emotion_detection_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing Cam and Using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model(r\"D:\\Mental Health\\models\\face_model.h5\")\n",
    "\n",
    "# Define the emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load the face detection model\n",
    "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_classifier.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        if np.sum([roi_gray]) != 0:\n",
    "            roi = roi_gray.astype('float') / 255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "            # Make predictions on the ROI using the model\n",
    "            preds = model.predict(roi)[0]\n",
    "\n",
    "            # Lookup the class\n",
    "            label = emotion_labels[preds.argmax()]\n",
    "            label_position = (x, y - 10)\n",
    "            accuracy = np.max(preds) * 100\n",
    "\n",
    "            # Display the label and accuracy on the frame\n",
    "            cv2.putText(frame, f'{label} ({accuracy:.2f}%)', label_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Emotion Detector', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summerization and Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dipender Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def create_summarizer():\n",
    "    \"\"\"\n",
    "    Creates and returns a summarization function.\n",
    "\n",
    "    Returns:\n",
    "        function: A function that takes text, max_length, and min_length as input and returns a summary.\n",
    "    \"\"\"\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "\n",
    "    def summarize(text, max_length=150, min_length=10):\n",
    "        \"\"\"\n",
    "        Summarizes the given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to summarize.\n",
    "            max_length (int): The maximum length of the summary.\n",
    "            min_length (int): The minimum length of the summary.\n",
    "\n",
    "        Returns:\n",
    "            str: The summarized text, or an error message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            return f\"Error during summarization: {e}\"\n",
    "\n",
    "    return summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Download necessary NLTK resources\u001b[39;00m\n\u001b[0;32m     11\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvader_lexicon\u001b[39m\u001b[38;5;124m'\u001b[39m, quiet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_enhanced_analyzer\u001b[39m():\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    Creates and returns a function that performs summarization, sentiment analysis,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    polarity, subjectivity, and other text analysis.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dipender Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dipender Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# %pip install textstat\n",
    "\n",
    "from transformers import pipeline\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import textstat\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def create_enhanced_analyzer():\n",
    "    \"\"\"\n",
    "    Creates and returns a function that performs summarization, sentiment analysis,\n",
    "    polarity, subjectivity, and other text analysis.\n",
    "    \"\"\"\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def analyze_text(text, max_length=100, min_length=10):\n",
    "        \"\"\"\n",
    "        Analyzes the given text, providing summarization, sentiment, polarity, subjectivity, and more.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        try:\n",
    "            summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
    "            results['summary'] = summary\n",
    "        except Exception as e:\n",
    "            results['summary'] = f\"Error during summarization: {e}\"\n",
    "\n",
    "        try:\n",
    "            sentiment_result = sentiment_analyzer(text)[0]\n",
    "            results['sentiment'] = sentiment_result['label']\n",
    "            results['sentiment_score'] = sentiment_result['score']\n",
    "        except Exception as e:\n",
    "            results['sentiment'] = f\"Error during sentiment analysis: {e}\"\n",
    "            results['sentiment_score'] = None\n",
    "\n",
    "        blob = TextBlob(text)\n",
    "        results['polarity'] = blob.sentiment.polarity\n",
    "        results['subjectivity'] = blob.sentiment.subjectivity\n",
    "        results['words'] = len(blob.words)\n",
    "        results['sentences'] = len(blob.sentences)\n",
    "        # results['noun_phrases'] = blob.noun_phrases\n",
    "        # results['word_counts'] = blob.word_counts\n",
    "        # results['sentiment_detailed'] = [sentence.sentiment for sentence in blob.sentences]\n",
    "\n",
    "        # Additional Extractions:\n",
    "\n",
    "        # Named Entity Recognition (NER)\n",
    "        doc = nlp(text)\n",
    "        # results['named_entities'] = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "        # POS Tagging\n",
    "        # tokens = nltk.word_tokenize(text)\n",
    "        # results['pos_tags'] = nltk.pos_tag(tokens)\n",
    "\n",
    "        # Sentiment Intensity (VADER)\n",
    "        results['vader_sentiment'] = sia.polarity_scores(text)\n",
    "\n",
    "        # Readability Score\n",
    "        results['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "        return results\n",
    "\n",
    "    return analyze_text\n",
    "\n",
    "# Example usage:\n",
    "analyze_function = create_enhanced_analyzer()\n",
    "\n",
    "text_to_analyze = \"\"\"\n",
    "Happiness is a fundamental goal that many people strive to achieve throughout their lives. It is a state of well-being characterized by positive or pleasant emotions ranging from contentment to intense joy, and it is often the ultimate aim of the decisions people make daily.\n",
    " However, the concept of happiness is subjective and varies from person to person. Some believe that happiness can be found in material wealth, while others find it in relationships, professional success, or personal fulfillment.\n",
    "\n",
    "To truly understand happiness, it is essential to recognize that it is not a constant state but rather a journey that requires effort and self-awareness. Aristotle once said, \"Happiness is the meaning and the purpose of life, the whole aim and end of human existence,\" highlighting the importance of happiness as a life goal.\n",
    " Achieving happiness involves understanding what it means to be happy and how to create it in one's life. This process is unique to each individual, as everyone has their own specific characteristics and defining traits that influence their path to happiness.\n",
    "\n",
    "One key aspect of happiness is the realization that it is not solely dependent on external factors such as wealth or status. While money can provide comfort and security, it does not guarantee happiness. The rich are often burdened with stress, anxiety, and insecurity, whereas the poor, despite their lack of material possessions, can find joy in simpler pleasures and stronger social bonds.\n",
    "\n",
    "Another important factor in achieving happiness is the development of positive relationships. Spending quality time with family and friends, expressing gratitude, and maintaining a positive outlook on life can significantly contribute to one's overall happiness.\n",
    " Positive thoughts and a positive mindset attract positive experiences, making it easier to sustain a state of happiness.\n",
    "\n",
    "Moreover, happiness can be cultivated through habits that promote well-being. Practicing mindfulness, transforming intense emotions into calmness and contentment, and focusing on the present moment can help maintain a state of happiness.\n",
    " Additionally, it is crucial to let go of past mistakes and not worry excessively about the future, as dwelling on negative thoughts can hinder one's ability to experience joy.\n",
    "\n",
    "In conclusion, happiness is a complex and multifaceted concept that requires personal effort and self-reflection. It is not about achieving perfection or external validation but rather about finding contentment and joy in one's life experiences and relationships. By understanding the true meaning of happiness and actively working towards it, individuals can lead a more fulfilling and joyful life.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "analysis_results = analyze_function(text_to_analyze)\n",
    "print(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What has brought you to therapy?\"\n",
    "text1 = input(\"Enter your answer here\")\n",
    "print(\"\\n\")\n",
    "summary1 = analyze_function(text1)\n",
    "\n",
    "print(\"Summary 1:\\n\", summary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"Are there any aspects of your daily life that you struggle with right now?\"\n",
    "text2 = input(\"Enter your answer here\")\n",
    "summary2 = analyze_function(text2, max_length=80, min_length=20) \n",
    "print(\"\\nSummary 2:\\n\", summary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"Where do you go for support when you need it? Do you have a network of supportive relationships with others?\"\n",
    "text3 = input(\"Enter your answer here\")\n",
    "summary3 = analyze_function(text3, max_length=80, min_length=20)\n",
    "print(\"\\nSummary 3:\\n\", summary3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 =\"How are your relationships with your family? How often are you in touch with them?\"\n",
    "text4 = input(\"Enter your answer here\")\n",
    "summary4 = analyze_function(text4)\n",
    "print(\"\\nSummary 4:\\n\", summary4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPzVQ0qFqSWN8k5kzgAzFuS",
   "gpuType": "T4",
   "mount_file_id": "1Q9MIh2SOcqXHOmUrLoXZzuXJeeDtaFCp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
